{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.18.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: onnx in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.16.1)\n",
      "Requirement already satisfied: gdown in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: torch in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torchvision in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.18.1+cu118)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: packaging in c:\\users\\rafay\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime) (5.27.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime) (1.12.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown) (3.14.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (1.7.1)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rafay\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\rafay\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U onnxruntime onnx gdown requests torch numpy torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "import sys\n",
    "import base64\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple\n",
    "import pickle\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print('cwd: ', cwd)\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[torch.Tensor]:\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "ModelStealingPub_id = '1nGzKrLOrb2w5E2ZZPySRQz0WxN77DTmL'\n",
    "\n",
    "if not downloaded:\n",
    "    gdown.download(f'https://drive.google.com/uc?id={ModelStealingPub_id}', 'ModelStealingPub.pt', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# dataset : TaskDataset = torch.load(\"/content/drive/MyDrive/ModelStealingPub.pt\")\n",
    "dataset : TaskDataset = torch.load(\"ModelStealingPub.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUESTING NEW API\n",
    "TOKEN = \"76282151\"  # Change this to your actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = requests.get(\"http://34.71.138.79:9090/stealing_launch\", headers={\"token\": TOKEN})\n",
    "answer = response.json()\n",
    "\n",
    "print(answer)  # {\"seed\": \"SEED\", \"port\": PORT}\n",
    "if 'detail' in answer:\n",
    "    sys.exit(1)\n",
    "\n",
    "# save the values\n",
    "SEED = str(answer['seed'])\n",
    "PORT = str(answer['port'])\n",
    "print(SEED)\n",
    "print(PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 4707839\n",
    "PORT = 9002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "images = deepcopy(dataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold the subarrays\n",
    "images1K = []\n",
    "\n",
    "# Divide the array into 13 subarrays each containing 1000 elements\n",
    "for i in range(0, len(images), 1000):\n",
    "    images1K.append(images[i:i + 1000])\n",
    "\n",
    "# Verify the result\n",
    "for idx, image1K in enumerate(images1K):\n",
    "    print(f\"Subarray {idx + 1}: Length = {len(image1K)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_stealing(images, port):\n",
    "    endpoint = \"/query\"\n",
    "    url = f\"http://34.71.138.79:{port}\" + endpoint\n",
    "    image_data = []\n",
    "    for img in images:\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        img.save(img_byte_arr, format='PNG')\n",
    "        img_byte_arr.seek(0)\n",
    "        img_base64 = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')\n",
    "        image_data.append(img_base64)\n",
    "\n",
    "    payload = json.dumps(image_data)\n",
    "    response = requests.get(url, files={\"file\": payload}, headers={\"token\": TOKEN})\n",
    "    if response.status_code == 200:\n",
    "        representation = response.json()[\"representations\"]\n",
    "        return representation\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Model stealing failed. Code: {response.status_code}, content: {response.json()}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the API\n",
    "outputs = []\n",
    "for batch in images1K:\n",
    "    batch_outputs = model_stealing(batch, port=PORT)\n",
    "    print(\"Batch done\")\n",
    "    outputs.extend(batch_outputs)\n",
    "    print(len(outputs))\n",
    "    time.sleep(65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output in a file\n",
    "with open('out.pickle', 'wb') as handle:\n",
    "    pickle.dump(outputs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the output from the file\n",
    "with open('out.pickle', 'rb') as handle:\n",
    "    outputs = pickle.load(handle)\n",
    "\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to convert grayscale to RGB\n",
    "def convert_to_rgb(image):\n",
    "    if image.mode == 'L':\n",
    "        # Convert grayscale to RGB\n",
    "        image = image.convert('RGB')\n",
    "    return image\n",
    "\n",
    "# Custom collate function to handle PIL images\n",
    "def collate_tensor(batch):\n",
    "    batch = [transforms.Resize((32, 32))(img) for img in batch]\n",
    "    batch = [convert_to_rgb(img) for img in batch]\n",
    "    batch = [transforms.ToTensor()(img) for img in batch]\n",
    "    return torch.stack(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of the Image Dataset for the Stolen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create a dataset instance\n",
    "stolen_dataset = ImageDataset(images=images, transform=transform,)\n",
    "\n",
    "# Create a DataLoader\n",
    "stolen_dataloader = DataLoader(dataset, batch_size=32, shuffle=False,collate_fn=collate_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN with 3 convolutional layers to replicate the model performance. MSE loss is chosen since it can produce the L2 differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StolenModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StolenModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        return x\n",
    "\n",
    "model = StolenModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "outputs_tensor = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "\n",
    "epochs = 10  \n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0  \n",
    "    for i, imgs in enumerate(stolen_dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_pred = model(imgs)\n",
    "        \n",
    "        start_idx = i * imgs.size(0)\n",
    "        end_idx = start_idx + imgs.size(0)\n",
    "        outputs_subset = outputs_tensor[start_idx:end_idx].to(device)\n",
    "        loss = criterion(outputs_pred, outputs_subset)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()  \n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(stolen_dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Avg Loss: {avg_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as ONNX\n",
    "path = 'stolen_model.onnx'\n",
    "dummy_input = torch.randn(1, 3, 32, 32, device=device)\n",
    "torch.onnx.export(model, dummy_input, path, export_params=True, input_names=[\"x\"])\n",
    "\n",
    "# Validate the model\n",
    "with open(path, \"rb\") as f:\n",
    "    model_data = f.read()\n",
    "    try:\n",
    "        stolen_model = ort.InferenceSession(model_data)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Invalid model, {e=}\")\n",
    "    try:\n",
    "        out = stolen_model.run(None, {\"x\": np.random.randn(1, 3, 32, 32).astype(np.float32)})[0][0]\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Some issue with the input, {e=}\")\n",
    "    assert out.shape == (1024,), \"Invalid output shape\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "path = 'stolen_model.onnx'\n",
    "# Send the model to the server\n",
    "response = requests.post(f\"http://34.71.138.79:9090/stealing\", files={\"file\": open(path, \"rb\")}, headers={\"token\": TOKEN, \"seed\": str(SEED)})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE ENDS HERE!!!\n",
    "BELOW CELLS SHOWCASE THE WORKS WHICH DIDN'T GIVE GOOD RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried to prepare augmented dataset according to \"Stolen Encoder\" but it didn't give good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class AugmentedImageDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.horizontal_flip = transforms.RandomHorizontalFlip(p=1.0)\n",
    "        self.converter = convert_to_rgb,\n",
    "        self.color = transforms.ColorJitter(hue=0.5, contrast=0.5, saturation = 0.5, brightness = 0.5 )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) * 3 \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx // 3\n",
    "        aug_type = idx % 3\n",
    "        image = self.images[original_idx]\n",
    "\n",
    "        if aug_type == 0:\n",
    "            # Original image\n",
    "            img = image\n",
    "        elif aug_type == 1:\n",
    "            # Horizontal flip\n",
    "            img = self.horizontal_flip(image)\n",
    "        else:\n",
    "            # Grayscale\n",
    "            img = self.color(image)\n",
    "        \n",
    "        img = self.converter(img)\n",
    "        img = self.to_tensor(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "# Create the dataset\n",
    "aug_dataset = AugmentedImageDataset(images)\n",
    "\n",
    "# Create a DataLoader\n",
    "aug_data_loader = DataLoader(aug_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried to prepare a Augmented Pair Dataset along with SimCLR but that didn't work either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "\n",
    "# Custom dataset for augmented pairs\n",
    "class AugmentedPairsDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "        self.augmentations = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image1 = self.transform(image)\n",
    "            image2 = self.transform(image)\n",
    "            image2 = self.augmentations(image2)\n",
    "        return image1, image2\n",
    "\n",
    "# SimCLR model with ResNet34 encoder and projection head\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, encoder, proj_dim=1024):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.projector(self.encoder(x1))\n",
    "        z2 = self.projector(self.encoder(x2))\n",
    "        return z1, z2\n",
    "\n",
    "# Contrastive loss function\n",
    "def contrastive_loss(z1, z2, temperature=0.5):\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    similarity_matrix = torch.matmul(z1, z2.T) / temperature\n",
    "\n",
    "    batch_size = z1.size(0)\n",
    "    mask = torch.eye(batch_size, device=z1.device).bool()\n",
    "\n",
    "    positives = similarity_matrix[mask].view(batch_size, -1)\n",
    "\n",
    "    negatives = similarity_matrix[~mask].view(batch_size, -1)\n",
    "\n",
    "    labels = torch.zeros(batch_size, device=z1.device, dtype=torch.long)\n",
    "    logits = torch.cat([positives, negatives], dim=1)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "aug_pairs_dataset = AugmentedPairsDataset(images, transform=transform)\n",
    "aug_pairs_data_loader = DataLoader(aug_pairs_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "aug_outputs = [torch.randn(128) for _ in range(len(images))] \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "aug_outputs_tensor = torch.stack(aug_outputs).to(device)\n",
    "\n",
    "encoder = resnet34(pretrained=True)\n",
    "model = SimCLR(encoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train(model, data_loader, aug_outputs_tensor, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_idx, (images1, images2) in enumerate(aug_pairs_data_loader):\n",
    "            images1, images2 = images1.to(device), images2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            z1, z2 = model(images1, images2)\n",
    "\n",
    "            start_idx = batch_idx * images1.size(0)\n",
    "            end_idx = start_idx + images1.size(0)\n",
    "            if end_idx > len(aug_outputs_tensor):\n",
    "                end_idx = len(aug_outputs_tensor)\n",
    "                start_idx = end_idx - images1.size(0)\n",
    "\n",
    "            aug_outputs_subset = aug_outputs_tensor[start_idx:end_idx]\n",
    "\n",
    "            aug_outputs_subset = aug_outputs_subset[:z1.size(0)]\n",
    "\n",
    "            loss_contrastive = contrastive_loss(z1, z2)\n",
    "\n",
    "            print(z1.size(),z2.size(),aug_outputs_subset)\n",
    "            similarity_loss = F.mse_loss(z1, aug_outputs_subset) + F.mse_loss(z2, aug_outputs_subset)\n",
    "\n",
    "            loss = loss_contrastive + similarity_loss\n",
    "            print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 99:   \n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(data_loader)}], Loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Train the model\n",
    "outputs_tensor = torch.tensor(outputs).to(device)\n",
    "train(model, aug_pairs_data_loader, outputs_tensor, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried MaskedAutoEncoder too with 3 convolutional layers. However, the results were terrible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MaskedAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Masking layer\n",
    "        self.mask = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        \n",
    "        masked_output = self.mask(x)\n",
    "        masked_output = masked_output * x  \n",
    "        return masked_output\n",
    "\n",
    "\n",
    "model = MaskedAutoEncoder().to(device)\n",
    "\n",
    "\n",
    "def soft_nearest_neighbors_loss(outputs_pred, outputs_subset):\n",
    "\n",
    "    outputs_pred_norm = F.normalize(outputs_pred, p=2, dim=1)\n",
    "    outputs_subset_norm = F.normalize(outputs_subset, p=2, dim=1)\n",
    "\n",
    "    cosine_similarity = torch.matmul(outputs_pred_norm, outputs_subset_norm.T)\n",
    "\n",
    "    loss = 1 - cosine_similarity\n",
    "    return loss.mean()\n",
    "\n",
    "# Convert outputs to tensor\n",
    "outputs_tensor = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "\n",
    "epochs = 25  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0  \n",
    "    for i, imgs in enumerate(stolen_dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_pred = model(imgs)\n",
    "        start_idx = i * len(imgs)\n",
    "        end_idx = min(start_idx + len(imgs), len(outputs))\n",
    "        outputs_subset = outputs_tensor[start_idx:end_idx].to(device)\n",
    "        loss = soft_nearest_neighbors_loss(outputs_pred, outputs_subset)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() \n",
    "\n",
    "    avg_loss = total_loss / len(stolen_dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Avg Loss: {avg_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
